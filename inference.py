import argparse
import json
import os
from datetime import datetime
from os import path
import torch
from transformers import AutoModelForTokenClassification, AutoTokenizer, TokenClassificationPipeline
from config import MODELS_DIR, DATASETS_DIR, SPECIAL_TOKENS
from utils.generic_utils import uniquify_filename

# Command line parameters
parser = argparse.ArgumentParser(description='NER with HuggingFace models')
parser.add_argument('model', type=str,
                    help='Name of a specific model previously saved inside "models" folder'
                         ' or name of an HuggingFace model')
parser.add_argument('doc', type=str,
                    help='Name of a specific .txt document present inside "datasets" folder. The document '
                         'should contain only plain text (without labels). Sequences should be separated by \n')
parser.add_argument('-noscores', action='store_true', help='If set, prediction scores wont be saved in output file')


def read_document(filepath: str):
    """
    Read a given document and return its content as a string
    :param filepath: path to the document
    :return: String representing the content of the document
    """
    with open(filepath, encoding='utf-8') as fp:
        doc = fp.read().replace("\n", " ")

    return doc


def process_predictions(predictions):
    """
    Process the predictions made by the model, removing special tokens and merging sub words (generated by
    the tokenization)
    :param predictions: list of dictionaries
        Each element of list refers to a specific token of the document
        each dictionary contains the word/token, the entity, the confidence score and start/end indexes
    :return: dictionary with key=(start_idx, end_idx) and value=(entity, text, score)
    """

    proc_predictions = []  # list of lists [[start_idx, end_idx, entity, word, score], ...]
    for token_pred in predictions:
        word, entity, score = token_pred['word'], token_pred['entity'], round(token_pred['score'], 3)
        start_idx, end_idx = token_pred['start'], token_pred['end']

        if word.startswith('##'):
            # A word starting with '##' means that it's a portion of the previous word
            # so we need to update the end index od the previous token if there is entity (!= 'O')
            proc_predictions[-1][1] = end_idx  # fix end index
            proc_predictions[-1][3] += word[2:]  # fix token

        else:  # Base case: add a list containing start_idx, end_idx, entity and score
            # If the current entity is a 'I-' then concatenate it to the previous one
            if proc_predictions and entity.startswith('I-') and entity[2:] == proc_predictions[-1][2]:
                proc_predictions[-1][3] += ' ' + word
                proc_predictions[-1][1] = end_idx
            else:
                proc_predictions.append([start_idx, end_idx, entity[2:], word, score])

    dict_predictions = {(el[0], el[1]): (el[2], el[3], el[4]) for el in proc_predictions}

    return dict_predictions


def convert_to_doccano(doc: str, predictions: dict):
    """
    Convert document and predictions into a json (Doccano) format
    :param doc: text document
    :param predictions: processed predictions of the model
    :return: json obj
    """
    labels = [[k[0], k[1], v[0]] for k, v in predictions.items()]
    docanno_json = {'text': doc, 'labels': labels}
    return docanno_json


def write_doccano_predictions(doccano_predictions: dict, output_dir: str, output_filename: str):
    """
    Write inference results on file (text with predictions).
    :param doccano_predictions: dict with key={'text', 'labels'}
    :param output_dir: path to folder where result will be saved
    :param output_filename: name of the results file that will be saved
    :return: void
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    output_filepath = uniquify_filename(path.join(output_dir, output_filename))
    with open(output_filepath, "w", encoding='utf-8') as writer:
        writer.write(json.dumps(doccano_predictions))


if __name__ == '__main__':
    args = parser.parse_args()
    model_name_or_path = args.model
    model_cache_dir = path.join('cache', args.model)
    is_a_presaved_model = len(args.model.split('_')) > 1

    # Load tokenizer
    tokenizer_name_or_path = path.join(MODELS_DIR, model_name_or_path) if is_a_presaved_model else model_name_or_path
    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=tokenizer_name_or_path,
                                              cache_dir=path.join(model_cache_dir, 'tokenizer'))

    # Load a specific, previously fine-tuned model or use one of the HuggingFace models
    model_name_or_path = path.join(MODELS_DIR, model_name_or_path) if is_a_presaved_model else model_name_or_path
    model = AutoModelForTokenClassification.from_pretrained(model_name_or_path,
                                                            cache_dir=path.join(model_cache_dir, 'model'),
                                                            return_dict=True)

    # Make predictions
    document = read_document(path.join(DATASETS_DIR, args.doc))
    nlp = TokenClassificationPipeline(task='ner',
                                      framework='pt',
                                      model=model,
                                      tokenizer=tokenizer,
                                      grouped_entities=False,
                                      device=torch.cuda.current_device() if torch.cuda.is_available() else -1,
                                      ignore_labels=[])
    predictions = nlp(document)

    # Process predictions
    processed_predictions = process_predictions(predictions)
    doccano_predictions = convert_to_doccano(document, processed_predictions)

    # Save predictions as Doccano (json1) file
    today_date_str = datetime.now().strftime("%Y%m%d")
    model_infer_dir = path.join(model_name_or_path, 'inferences')
    predictions_filename = today_date_str + '_' + args.doc.split('.', 1)[0] + '.json1'
    write_doccano_predictions(doccano_predictions,
                              output_dir=model_infer_dir,
                              output_filename=predictions_filename)

    """
    #
    #OLD CODE -> to remove after tests
    #
    
    # Generate a new document (as list of sequences) with the labels predicted by the model
    document_with_preds = []

    for seq_pred in predictions if len(document) > 1 else [predictions]:
        seq_with_preds = []

        for token_pred in seq_pred:
            word, entity, score = token_pred['word'], token_pred['entity'], round(token_pred['score'], 3)
            if word in SPECIAL_TOKENS:
                # If word is a special token, we just skip it
                pass

            elif word.startswith('##'):
                # A word starting with '##' means that it's a portion of the previous
                # word so we concatenate the '##' word with the previous one
                seq_with_preds[-1][0] += word[2:]

            else:
                # Base case: add a tuple containing word, tag and score
                seq_with_preds.append([word, entity, score])

        document_with_preds.append(seq_with_preds)

    # Writing inference result
    today_date_str = datetime.now().strftime("%Y%m%d")
    model_infer_dir = path.join(model_name_or_path, 'inferences')
    infer_result_filename = today_date_str + '_' + args.doc.split('.', 1)[0] + '.txt'
    write_inference_result(document_with_preds, model_infer_dir, infer_result_filename,
                           include_scores=not args.noscores)
    print(f"Inference result saved inside {model_infer_dir}.")
    """
